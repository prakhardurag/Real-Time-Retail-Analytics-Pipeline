{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97005ace",
   "metadata": {},
   "source": [
    "SPARK NOTEBOOK SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b844466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col, expr, to_timestamp, udf\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS = \"localhost:9092\"  # Or \"kafka:9092\" if using Docker\n",
    "KAFKA_TOPIC = \"transactions\"\n",
    "\n",
    "POSTGRES_JDBC_URL = \"jdbc:postgresql://postgres:5432/retail\"\n",
    "POSTGRES_USER = \"admin\"\n",
    "POSTGRES_PASSWORD = \"admin\"\n",
    "POSTGRES_DRIVER = \"org.postgresql.Driver\"\n",
    "\n",
    "FACT_TABLE = \"fact_transactions\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee0d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToPostgresStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,org.postgresql:postgresql:42.6.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_products_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", POSTGRES_JDBC_URL) \\\n",
    "    .option(\"dbtable\", \"dim_products\") \\\n",
    "    .option(\"user\", POSTGRES_USER) \\\n",
    "    .option(\"password\", POSTGRES_PASSWORD) \\\n",
    "    .option(\"driver\", POSTGRES_DRIVER) \\\n",
    "    .load()\n",
    "\n",
    "dim_location_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", POSTGRES_JDBC_URL) \\\n",
    "    .option(\"dbtable\", \"dim_location\") \\\n",
    "    .option(\"user\", POSTGRES_USER) \\\n",
    "    .option(\"password\", POSTGRES_PASSWORD) \\\n",
    "    .option(\"driver\", POSTGRES_DRIVER) \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b49cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"store_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1264af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers = \"kafka:9092\"\n",
    "\n",
    "raw_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "  .option(\"subscribe\", \"transactions\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_df = raw_df.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "\n",
    "json_df = value_df.select(from_json(col(\"json_str\"), kafka_schema).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Define validity condition\n",
    "valid_condition = (\n",
    "    col(\"transaction_id\").isNotNull() &\n",
    "    col(\"quantity\").isNotNull() &\n",
    "    col(\"payment_method\").isNotNull() &\n",
    "    (col(\"price\") >= 0)\n",
    ")\n",
    "\n",
    "# Clean data\n",
    "clean_df = json_df \\\n",
    "    .filter(valid_condition) \\\n",
    "    .withColumn(\"transaction_timestamp\", to_timestamp(\"timestamp\")) \\\n",
    "    .drop(\"timestamp\")\n",
    "\n",
    "# Bad records\n",
    "bad_df = json_df \\\n",
    "    .filter(~valid_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = clean_df \\\n",
    "    .withColumnRenamed(\"price\", \"total_price\") \\\n",
    "    .join(dim_products_df.withColumnRenamed(\"price\", \"unit_price\"), on=\"product_id\", how=\"inner\") \\\n",
    "    .join(dim_location_df, on=\"store_id\", how=\"inner\") \\\n",
    "    .select(\n",
    "        col(\"transaction_id\"),\n",
    "        col(\"product_id\"),\n",
    "        col(\"name\").alias(\"product_name\"),\n",
    "        col(\"category\").alias(\"product_category\"),\n",
    "        col(\"store_id\"),\n",
    "        col(\"city\"),\n",
    "        col(\"state\"),\n",
    "        col(\"country\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"total_price\"),\n",
    "        col(\"payment_method\"),\n",
    "        col(\"transaction_timestamp\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_df.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"/tmp/bad_data/transactions/\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/bad_data\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1811f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bad_to_postgres(batch_df, epoch_id):\n",
    "    batch_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", POSTGRES_JDBC_URL) \\\n",
    "        .option(\"dbtable\", \"bad_transactions\") \\\n",
    "        .option(\"user\", POSTGRES_USER) \\\n",
    "        .option(\"password\", POSTGRES_PASSWORD) \\\n",
    "        .option(\"driver\", POSTGRES_DRIVER) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_postgres(batch_df, epoch_id):\n",
    "    try:\n",
    "        batch_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", POSTGRES_JDBC_URL) \\\n",
    "            .option(\"dbtable\", FACT_TABLE) \\\n",
    "            .option(\"user\", POSTGRES_USER) \\\n",
    "            .option(\"password\", POSTGRES_PASSWORD) \\\n",
    "            .option(\"driver\", POSTGRES_DRIVER) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to write batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1da3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start clean data stream\n",
    "clean_query = joined_df.writeStream \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/transactions_fact\") \\\n",
    "    .start()\n",
    "\n",
    "# Start bad data stream\n",
    "bad_query = bad_df.writeStream \\\n",
    "    .foreachBatch(write_bad_to_postgres) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/bad_transactions\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for both to terminate\n",
    "clean_query.awaitTermination()\n",
    "bad_query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
